# Predictor de Flujo de Pasajeros del Subte de Buenos Aires üöá

Este proyecto tiene como objetivo predecir el flujo de pasajeros del subte de Buenos Aires, proporcionando una soluci√≥n para optimizar la frecuencia de los vagones y reducir la congesti√≥n en las horas pico. Al conocer de antemano la cantidad de pasajeros que utilizar√°n el servicio, se pueden tomar decisiones informadas para gestionar mejor los recursos y garantizar una experiencia de viaje m√°s c√≥moda. Entre otras posibles ventajas puede ser:

- **Mejora de la Planificaci√≥n del Servicio**: Con las predicciones de pasajeros, se puede ajustar la programaci√≥n de trenes para maximizar la eficiencia del servicio.
- **Optimizaci√≥n de Recursos**: Ayuda a reducir costos operativos al evitar el env√≠o de trenes vac√≠os o con poca demanda.
- **Incremento en la Satisfacci√≥n del Cliente**: Al reducir la congesti√≥n, se mejora la experiencia del usuario, lo que puede llevar a un aumento en la utilizaci√≥n del servicio.

## üóÇÔ∏è Estructura del Proyecto

El proyecto se organiza en cuatro notebooks como recomienda la empresa ***Hopsworks***, cada una desempe√±ando un papel crucial en el flujo de trabajo:

1. **Backfill.ipynb**: 
   - Realiza el procesamiento inicial de los datos, incluyendo la descarga desde la web oficial del Gobierno de la Ciudad de Buenos Aires.
   - Los datos se suben a la feature store de Hopsworks, donde quedan disponibles para su posterior an√°lisis.

2. **Feature Pipeline.ipynb**: 
   - Obtiene los datos de la √∫ltima hora y los sube a la feature store, asegurando que siempre se tenga acceso a informaci√≥n actualizada para el entrenamiento del modelo.

3. **Training Pipeline.ipynb**: 
   - Estructura los datos en un formato adecuado para el entrenamiento del modelo XGBoost, que predice el flujo de pasajeros en las pr√≥ximas tres horas.
   - El modelo se guarda en el registro de modelos de COMET_ML para su gesti√≥n y seguimiento.

4. **Inference.ipynb**: 
   - Realiza predicciones sobre el flujo de pasajeros tomando en cuenta los datos desde el tiempo actual hasta 14 d√≠as atr√°s.
   - Las predicciones se suben nuevamente a la feature store para ser consumidas por un frontend desarrollado en Streamlit.

## ‚öôÔ∏è Implementaci√≥n de MLOps

Para garantizar las mejores pr√°cticas en MLOps, se utilizan **GitHub Actions** para automatizar el proceso de ejecuci√≥n de las notebooks de features y inference cada hora, manteniendo as√≠ los datos actualizados.

Adem√°s, el proyecto utiliza un entorno **Poetry** para gestionar las dependencias y ejecutar el c√≥digo de manera eficiente.

## üìä Visualizaci√≥n

La visualizaci√≥n de los datos y predicciones se realiza utilizando **Streamlit Community Cloud**, donde se ha implementado un c√≥digo en Python que obtiene las caracter√≠sticas actuales, predichas e hist√≥ricas de los datos. Este frontend permite interactuar con los datos de manera sencilla y visual.

- Los datos actuales, desde mayo hasta la fecha, son simulados utilizando los datos del mismo per√≠odo del a√±o anterior, lo que permite evaluar el comportamiento del modelo con datos cercanos a la realidad actual.
